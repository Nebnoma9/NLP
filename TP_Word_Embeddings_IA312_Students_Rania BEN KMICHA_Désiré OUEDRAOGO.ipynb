{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors : Rania BEN KMICHA , Désiré OUEDRAOGO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrôle de version\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "# Packages nécessaires\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# A la première utilisation de nltk, télécharger les données nécessaires\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Representations \n",
    "\n",
    "### Word Embeddings : Distributed representations via the distributional hypothesis \n",
    "\n",
    "**Goal**: We will try to obtain dense representations (as vectors of real numbers) of words (and possibly sentences). These representations are intended to be distributed: they are non-local representations. We represent an object as a combination of *features*, as opposed to the attribution of a dedicated symbol: see the founding work of Geoffrey Hinton, among others, on the subject: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
    "\n",
    "The term *distributed* representations is very general, but is what we are looking for. The challenge is therefore to be able to build, automatically, such representations.\n",
    "\n",
    "**Underlying idea**: It is based on the distributional hypothesis: contextual information is sufficient to obtain a viable representation of linguistic objects.\n",
    " - For a large class of cases [...] the meaning of a word is its use in the language.\" Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
    " - You shall know a word by the company it keeps, Firth.\n",
    "\n",
    "Thus, a word can be characterized by the words that accompany it, via co-occurrence counts. Two words with a similar meaning will have a similar contextual distribution and are therefore more likely to appear in similar contexts. This hypothesis can be used as a justification for the application of statistics to semantics (information extraction, semantic analysis). It also allows some form of generalization: we can assume that the information we have about a word will be generalized to words with a similar distribution. \n",
    "\n",
    "**Motivation**: The goal is to obtain distributed representations in order to be able to effectively**:\n",
    "- Directly perform a semantic surface analysis.\n",
    "- Use it as a source of information for other language-related models and applications, especially for sentiment analysis. \n",
    "\n",
    "\n",
    "**Terminology**: Be careful not to confuse the idea of *distributed* and *distributional* representation. The latter generally indicates (for words) that the representation has been obtained strictly from co-occurrence counts, whereas additional information (document labels, part of speech tags, ...) can be used to build distributed representations. \n",
    "The models that allow to build these dense representations, in the form of vectors, are often called *vector spaces models*. These representations are also regularly called *word embeddings*, because the words are embedded in a vector space. In French, we often find the term *word embedding* or *lexical embedding*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting representations: counts of occurrences and co-occurrences\n",
    "\n",
    "Depending on the type of corpus available, different types of distributional information can be obtained. If we have access to a collection of documents, we can thus choose to count the number of occurrences of each word in each document, to obtain a $words \\times documents$ matrix: it is on this principle that **Tf-Idf** is built. We will now look at a more general case: we have a large amount of data in text form, and we want to obtain representations of words in the form of vectors of reduced size, without the need to divide them into documents or categories. \n",
    "\n",
    "Suppose we have a corpus containing $T$ different words. We will construct a $\\mathbf{M}$ matrix of size $T \\times T$ which will contain the number of co-occurrences between words. There will be different factors to consider when constructing this matrix: \n",
    "\n",
    "- How do you define the 'context' of a word - context which will tell you what terms co-occur with that word?\n",
    "\n",
    "We can choose to use different scales: the document, the sentence, the nominal group, or simply a window of $k$ words, depending on the information we want to capture.\n",
    "\n",
    "\n",
    "- How do we quantify the importance of the counts? \n",
    "\n",
    "$\\rightarrow$ For example, we can give a decreasing weight to a co-occurrence according to the distance between the two words concerned ($\\frac{1}{d+1}$ for a separation by $d$ words).\n",
    "\n",
    "\n",
    "- Should we keep all the words that appear in the corpus? \n",
    "\n",
    "$\\rightarrow$ Usually not. We will see that for large corpora, the number $T$ of different words is huge. Second, even if the number of words is reasonable, we will have very little distributional information on the rarest words, and the representation obtained will be of poor quality. We will have to ask ourselves how to filter these words, and how to treat the words we choose not to represent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "Let's look at the following text:\n",
    "\n",
    "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
    "\n",
    "We choose to define the context of a word as the sentence to which it belongs, and to not use any weighting.\n",
    "We obtain the following matrix: \n",
    "\n",
    "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
    "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
    "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
    "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
    "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
    "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
    "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
    "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
    "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
    "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
    "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice: get a Vocabulary.\n",
    "\n",
    "To begin, we will implement separately a function returning the vocabulary. Here we will have to be able to control its size, by indicating a maximum number of words. We add, at the end, an \"unknown\" word that will replace all the words that do not appear in our \"limited\" vocabulary. \n",
    "\n",
    "**Remarks:**\n",
    "- Use tokenization to obtain words from a document !\n",
    "- Add a special token ```<UNK>``` to deal with out-of-vocabulary words: even if you don't put a limit, you might encounter new words when working with new data. \n",
    "- You need to count words and sort them by frequency in order to only keep the most frequent ones. It is not necessary to count 'unknown' words. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(corpus, count_threshold=5, voc_threshold=0):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency \n",
    "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
    "                                             values: word counts in the corpus\n",
    "    \"\"\"\n",
    "    n_samples = len(corpus)\n",
    "    word_counts = {}\n",
    "    words = []\n",
    "    for text in corpus:\n",
    "        words += list(clean_and_tokenize(text)) # list of all words\n",
    "    for word in words:\n",
    "        if word not in word_counts.keys():\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word] += 1\n",
    "    #sorted()\n",
    "    word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)} \n",
    "    filtered_word_counts={}\n",
    "    for k,v in word_counts.items():\n",
    "        if voc_threshold > 0 :\n",
    "            if v>=count_threshold and len(filtered_word_counts.keys()) < voc_threshold:\n",
    "                filtered_word_counts[k] = v\n",
    "        else :\n",
    "            if v>=count_threshold:\n",
    "                filtered_word_counts[k] = v\n",
    "       \n",
    "    vocabulary = {}\n",
    "    vocabulary_word_counts = filtered_word_counts\n",
    "    vocabulary_word_counts['UNK']=0\n",
    "    voc = list(vocabulary_word_counts.keys())\n",
    "    vocabulary = dict(zip(voc, range(len(voc))))  \n",
    "    \n",
    "    return vocabulary, vocabulary_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
      "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n",
      "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
      "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example for testing:\n",
    "\n",
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 0)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B Obtaining co-occurences:\n",
    "\n",
    "The function takes as input the corpus (a list of strings, corresponding to documents/sentences) and a vocabulary, as well as the size of the context window. We can also implement the simplest solution: the context of a word being the full document to which it belongs. \n",
    "Finally, we can implement the possibility of making linearly decrease the importance of the context of a word when getting further from the input word.\n",
    "\n",
    "**Remark:**\n",
    "- The matrix we are building is symmetric: we can only build half of it !\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(vocabulary)\n",
    "    M = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Obtenir la phrase:\n",
    "        sent = clean_and_tokenize(sent)\n",
    "        # Obtenir les indexs de la phrase grace au vocabulaire: \n",
    "        sent_idx = []\n",
    "        for word in sent:\n",
    "            if vocabulary.get(word) != None : \n",
    "                sent_idx.append(vocabulary.get(word))\n",
    "            else:\n",
    "                sent_idx.append(vocabulary.get('UNK'))\n",
    "        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n",
    "        for i, idx_i in enumerate(sent_idx):\n",
    "            # On vérifie que le mot est reconnu par le vocabulaire:\n",
    "            if idx_i > -1:\n",
    "                # Si on considère un contexte limité:\n",
    "                if window > 0:\n",
    "                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n",
    "                    l_ctx_idx = [sent_idx[j] for j in range(max(0,i - window),i)]\n",
    "                    # A compléter\n",
    "                    \n",
    "                # Si on considère que le contexte est la phrase entière:\n",
    "                else:\n",
    "                    # La liste qui contient le contexte à gauche du mot est plus facile à créer:\n",
    "                    l_ctx_idx = sent_idx[:i]\n",
    "                    # A compléter\n",
    "                # On parcourt cette liste et on update M[i,j]:    \n",
    "                for j, idx_j in enumerate(l_ctx_idx):\n",
    "                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n",
    "                    if idx_j > -1:\n",
    "                        # Calcul du poids:\n",
    "                        if distance_weighting:\n",
    "                            weight = 1.0 \n",
    "                            # A compléter *\n",
    "                            if i-j != 0:\n",
    "                               weight = 1/abs(i-j)\n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        M[idx_i, idx_j] += weight * 1.0\n",
    "                        M[idx_j, idx_i] += weight * 1.0\n",
    "    return M  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 7. 6. 3. 3. 2. 2. 1. 1. 0.]\n",
      " [7. 2. 6. 2. 2. 3. 3. 1. 1. 0.]\n",
      " [6. 6. 0. 2. 2. 2. 2. 1. 1. 0.]\n",
      " [3. 2. 2. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [3. 2. 2. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [2. 3. 2. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [2. 3. 2. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(co_occurence_matrix(corpus, voc, 0, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C Application to a real data set\n",
    "\n",
    "We're going to work with the **imdb** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouedr\\AppData\\Local\\Temp\\ipykernel_20320\\3405308050.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.ones(len(texts), dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "filenames_neg = sorted(glob(os.path.join('.', 'aclImdb', 'train', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(os.path.join('.', 'aclImdb', 'test', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick study of the data\n",
    "\n",
    "We would like to get an idea of what's in these film reviews before we proceed. So we'll get the vocabulary (in full) and represent the frequencies of the words, in order (be careful, you'll have to use a logarithmic scale): we should find back Zipf's law. This will give us an idea of the size of the vocabulary we will be able to choose: it's a matter of making a compromise between the necessary resources (size of the objects in memory) and the amount of information we can get from them (rare words can bring a lot of information, but it's difficult to learn good representations of them, because they are rare!). \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 93231\n",
      "Part of the corpus by taking the \"x\" most frequent words ?\n",
      "x =  500\n",
      " 69.124630 %\n"
     ]
    }
   ],
   "source": [
    "# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n",
    "vocab, word_counts = vocabulary(texts)\n",
    "#\n",
    "# \n",
    "#\n",
    "\n",
    "# We can for example use the function plt.scatter()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Word counts versus rank')\n",
    "plt.scatter(vocab.values(), word_counts.values())\n",
    "# \n",
    "#\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# We would like to know how much of the data is represented by the 'k' most frequent words\n",
    "print('Vocabulary size: %i' % len(vocab))\n",
    "print('Part of the corpus by taking the \"x\" most frequent words ?')\n",
    "#\n",
    "while 1:\n",
    "    x=int(input())\n",
    "    if x < len(vocab):\n",
    "        break\n",
    "\n",
    "lst_of_fword_occ = list(word_counts.values())[:x]\n",
    "sum_of_occ = sum(lst_of_fword_occ)\n",
    "result = sum_of_occ/sum(list(word_counts.values()))\n",
    "print(\"x = \",x)\n",
    "print(\" {0:1f} %\".format(result*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result of the analysis**: we can be satisfied with a vocabulary of 10,000 or even 5,000 words - this is important, because it will determine the size of the objects we will manipulate.\n",
    "\n",
    "#### Obtaining the matrices:\n",
    "We can now get the co-occurence matrices with parameters of our choosing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 5001)\n",
      "(5001, 5001)\n"
     ]
    }
   ],
   "source": [
    "vocab_5k, word_counts_5k = vocabulary(texts, 0, 5000)\n",
    "M5dist = co_occurence_matrix(texts, vocab_5k, window=5, distance_weighting=True)\n",
    "M20 = co_occurence_matrix(texts, vocab_5k, window=20, distance_weighting=False)\n",
    "print(M5dist.shape)\n",
    "print(M20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.34404393e+03 3.80292974e+03 4.26779039e+03 ... 3.01301407e+00\n",
      " 1.58905923e+00 1.77953231e+04]\n"
     ]
    }
   ],
   "source": [
    "print(M5dist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "[ 9.36018576  9.09404357  8.17571046 ...  0.          0.\n",
      " 12.15935411]\n",
      "[2.621e+03 1.640e+03 1.513e+03 ... 2.000e+00 0.000e+00 3.763e+03]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_5k['cinema'])\n",
    "print(M5dist[429])\n",
    "print(M20[429])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector comparison \n",
    "\n",
    "We can use these very large-dimensional vectors for a very simple semantic analysis: for example, by looking for the nearest neighbors of a words. However, we need to be careful to the distance that we use (euclidean, cosine). Vector normalization can also play a role; in any way, we need to not over-interpret this type of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec un contexte large, sans prendre en compte la distance entre les mots:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['very', 'what', 'more', 'even', 'no', 'time', 'there', 'story', 'only']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['very', 'great', 'decent', 'pretty', 'not', 'funny', 'cool', 'really', 'just']]\n",
      "\n",
      "Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['great', 'very', 'bad', 'what', 'story', 'time', 'well', 'made', 'its']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'very', 'simple', 'fun', 'decent', 'little', 'strange', 'funny', 'perfect']]\n"
     ]
    }
   ],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u))\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def print_neighbors(distance, voc, co_oc, mot, k=10):\n",
    "    inv_voc = {id: w for w, id in voc.items()}\n",
    "    neigh = NearestNeighbors(n_neighbors=k, algorithm='brute', metric=distance)\n",
    "    neigh.fit(co_oc) \n",
    "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n",
    "    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n",
    "    print([[inv_voc[i] for i in s[1:]] for s in ind])\n",
    "    \n",
    "print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n",
    "print_neighbors(euclidean, vocab_5k, M20, 'good')\n",
    "print_neighbors(cosine, vocab_5k, M20, 'good')\n",
    "print(\"\")\n",
    "print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n",
    "print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n",
    "print_neighbors(cosine, vocab_5k, M5dist, 'good') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector re-weighting:\n",
    "\n",
    "Similarly as before, we may want to alter the representations to obtain better features - depending on what use we will have for them.\n",
    "\n",
    "**Normalization**: Very easy: we want to cancel the influence of the magnitude of the counts on the representation.\n",
    "\n",
    "$$\\mathbf{m_{normalized}} = \\left[ \n",
    "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\ldots\n",
    "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "\\right]$$\n",
    " \n",
    "**Pointwise Mutual Information**: The aim is to assess the extent to which the co-occurrence of the two terms is *unexpected*. This measure is the ratio of the joint probability of the two words and the product of their individual probabilities:\n",
    "$$\n",
    "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
    "$$\n",
    "The joint probability of the two words corresponds to the number of times they are observed together, divided by the total number of co-occurrences in the corpus: \n",
    "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "The individual probability of a word simply corresponds to its frequency, which can be calculated by counting all co-occurrences where that word appears:\n",
    "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "Hence,\n",
    "$$ \n",
    "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
    "$$\n",
    "We thus calculate the discrepancy between the observation we have made in our corpus and the frequency of appearance of these terms if we consider them independent - i.e. we assume that their co-occurrence is a coincidence.\n",
    "\n",
    "The main problem with this measure is that it is not adapted to the case where no co-occurrence is observed. Since the PMI is supposed to return a positive quantity if more co-occurrences are observed than expected, and a negative quantity if fewer co-occurrences are observed, we cannot choose to replace $\\log(0)$ by $0$. A commonly used solution is to use the **Positive PMI**, which sets all negative values to $0$.\n",
    " \n",
    " $$ \\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
    " \\begin{cases}\n",
    " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
    " 0 & \\textrm{otherwise}\n",
    " \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(co_oc, positive=True):\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    sum_tot = sum_vec.sum()\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                 \n",
    "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "    if positive:\n",
    "        pmi[pmi < 0] = 0.0\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la PPMI:\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['movie', 'this', 'it', 'the', 'is', 'was', 'but', 'film', 'one']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'decent', 'but', 'and', 'acting', 'some', 'bad', 'were', 'overall']]\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['but', 'is', 'movie', 'and', 'it', 'this', 'the', 'not', 'that']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'bad', 'pretty', 'acting', 'decent', 'but', 'movie', 'very', 'overall']]\n"
     ]
    }
   ],
   "source": [
    "PPMI5 = pmi(M5dist)\n",
    "PPMI20 = pmi(M20)\n",
    "\n",
    "print(\"Avec la PPMI:\")    \n",
    "print_neighbors(euclidean, vocab_5k, PPMI5, 'good')\n",
    "print_neighbors(cosine, vocab_5k, PPMI5, 'good')\n",
    "print_neighbors(euclidean, vocab_5k, PPMI20, 'good')\n",
    "print_neighbors(cosine, vocab_5k, PPMI20, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representations through count-based methods: reducing the dimension of co-occurence matrices\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "The goal is not only to reduce the size of the data representation (then, we will work with smaller pre-determined sized representations, instead of working with vectors of the size of the vocabulary) but also showcase higher-level relationships between words: by reducing their representations to the *most important dimensions* of the data, we end up *generalizing* some properties of words.\n",
    "\n",
    "#### Dimension reduction via SVD \n",
    "\n",
    "A matrix is a linear transformation: applying a SVD to it is decomposing our linear transformation in a product of linear transformations of different types. We are changing the basis and replacing our data in a different space, using **eigenvectors**:\n",
    "\n",
    "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
    "\n",
    "Matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, and $\\mathbf{V}$ have the following properties:\n",
    "- $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ and $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). They contain the eingenvectors to the left and to the right of $\\mathbf{M}$.\n",
    "- $\\mathbf{\\lambda}$ is a diagonal matrix: careful, it is not necessarily square. diagonal coefficient are the eigenvalues of $\\mathbf{M}$.\n",
    "\n",
    "Then, the *most important dimensions* correspond to the highest eigenvalues. Reducing our data to a dimension $k$ corresponds to only keeping vectors corresponding to the $k$ first eigenvalues - which in turns is equivalent to keeping only the first $k$ vectors of $U$. \n",
    "We use ```TruncatedSVD``` from ```scikit-learn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 300)\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['great', 'very', 'bad', 'what', 'story', 'time', 'well', 'made', 'its']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'very', 'simple', 'decent', 'perfect', 'terrific', 'fun', 'strange', 'little']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "SVDEmbeddings = svd.fit_transform(M5dist)\n",
    "print(SVDEmbeddings.shape)\n",
    "SVDEmbeddings[vocab_5k['UNK']]\n",
    "\n",
    "print_neighbors(euclidean, vocab_5k, SVDEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, SVDEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When applying this method to the count matrix $\\mathbf{M}$ of dimension $T \\times D$, where $\\mathbf{M}_{t,d}$ contains the number of occuerences of the word $t$ in the document $d$, we obtain the method called **Latent Semantic Analysis**, which is used to detect latent (semantic) components allowing to group documents together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization in two dimensions\n",
    "\n",
    "We will now use **principal components analysis** (PCA) to visualize our data in two dimensions. This is equivalent to applying SVD to the covariance matrix of the data, in order for the principal components to be independant from each other an maximize the variance of the data. We use the class ```PCA``` from ```scikit-learn```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(M5dist)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'award']\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(Norm5)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'award']\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representations through prediction-based methods: commonly used algorithms\n",
    "\n",
    "The idea here is to define a set of representations ${w_{i}}_{i=1}^{V}$, of predefined dimension $d$ (here, we will work with $d = 300$), for all the words $i$ of the vocabulary $V$ - then **train** these representations to match what we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove\n",
    "\n",
    "The objective defined by Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) is to learn from the vectors $w_{i}$ and $w_{k}$ so that their scalar product corresponds to the logarithm of their **Pointwise Mutual Information**: \n",
    "\n",
    "\n",
    "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
    "\n",
    "\n",
    "In the article, this objective is carefully justified by a reasoning about the operations one wants to perform with these vectors and the properties they should have - in particular, symmetry between rows and columns (see the article for more details).  \n",
    "The final goal obtained is the following, where $M$ is the co-occurrence matrix:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
    "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
    "  \n",
    " \n",
    "Here, $f$ is a *scaling* function that reduces the importance of the most frequent co-occurrence counts: \n",
    "\n",
    "\n",
    "$$f(x) \n",
    "\\begin{cases}\n",
    "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
    "1 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "Usually, we choose $\\alpha=0.75$ and $x_{\\max} = 100$, although these parameters may need to be changed depending on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the gensim API to retrieve pre-trained representations (It is normal that the loading is long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the embedding matrix this way, and check its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 300)\n"
     ]
    }
   ],
   "source": [
    "loaded_glove_embeddings = loaded_glove_model.vectors\n",
    "print(loaded_glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are $400,000$ words represented, and that the embeddings are of size $300$. We define a function that returns, from the loaded model, the vocabulary and the embedding matrix according to the structures we used before. We add, here again, an unknown word \"UNK\" in case there are words in our data that are not part of the $400,000$ words represented here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_voc_and_embeddings(glove_model):\n",
    "    voc = {word : index for word, index in enumerate(glove_model.index_to_key)}\n",
    "    voc['UNK'] = len(voc)\n",
    "    embeddings = glove_model.vectors\n",
    "    return voc, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the representations loaded here and the ones produced with word2vec, the same vocabulary should be used. For this purpose, I reuse the following code to create a $5000$ word vocabulary from the data, and I add at the end a function that returns the matrix of representations loaded with Glove for these $5000$ words only, in the right order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_adapted_embeddings(glove_model, input_voc):\n",
    "    keys = {i: glove_model.key_to_index.get(w, None) for w, i in input_voc.items()}\n",
    "    index_dict = {i: key for i, key in keys.items() if key is not None}\n",
    "    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n",
    "    for i, ind in index_dict.items():\n",
    "        embeddings[i] = glove_model.vectors[ind]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input the model loaded using the Gensim API, as well as a vocabulary we created ourselves, and returns the embedding matrix from the loaded model, for the words in our vocabulary and in the right order.\n",
    "Note: unknown words are represented by a vector of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(GloveEmbeddings.shape)\n",
    "GloveEmbeddings[vocab_5k['UNK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['better', 'well', 'always', 'really', 'sure', 'way', 'so', 'but', 'excellent']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouedr\\AppData\\Local\\Temp\\ipykernel_20320\\4131742084.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  return u / np.sqrt(u.dot(u))\n"
     ]
    }
   ],
   "source": [
    "print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "\n",
    "**Reminder: The skip-gram model**\n",
    "\n",
    "The basic skip-gram model estimates the probabilities of a pair of words $(i, j)$ to appear together in data:\n",
    "\n",
    "$$P(j \\mid i) = \\frac{\\exp(w_{i} c_{j})}{\\sum_{j'\\in V}\\exp(w_{i} c_{j'})}$$\n",
    "\n",
    "\n",
    "where $w_{i}$ is the lign vector (of the word) $i$ and $c_{j}$ is the column vector (of a context word) $j$. The objective is to minimize the following quantity:\n",
    "\n",
    "\n",
    "$$ -\\sum_{i=1}^{m} \\sum_{k=1}^{|V|} \\textbf{1}\\{o_{i}=k\\} \\log \\frac{\\exp(w_{i} c_{k})}{\\sum_{j=1}^{|V|} \\exp(w_{i} c_{j})}$$\n",
    "\n",
    "\n",
    "where $V$ is the vocabulary.\n",
    "The inputs $w_{i}$ are the representations of the words, which are updated during training, and the output is an *one-hot* $o$ vector, which contains only one $1$ and $0$. For example, if `good` is the 47th word in the vocabulary, the output $o$ for an example or `good` is the word to predict will consist of $0$s everywhere except $1$ in the 47th position of the vector. `good` will be the word to predict when the input $w$ is a word in its context.\n",
    "We therefore obtain this output with standard softmax - we add a bias term $b$ .\n",
    "\n",
    "\n",
    "$$ o = \\textbf{softmax}(w_{a}C + b)$$\n",
    "\n",
    "\n",
    "If we use the set of representations for the whole vocabulary (the matrix $W$) as input, we get:\n",
    "\n",
    "\n",
    "$$ O = \\textbf{softmax}(WC + b)$$\n",
    "\n",
    "\n",
    "and so we come back to the central idea of all our methods: we seek to obtain word representations from co-occurrence counts. Here, we train the parameters contained in $W$ and $C$, two matrices representing the words in reduced dimension (300) so that their scalar product is as close as possible to the co-occurrences observed in the data, using a maximum likelihood objective.\n",
    "\n",
    "#### Skip-gram with negative sampling\n",
    "\n",
    "The training of the skip-gram model implies to calculate a sum on the whole vocabulary, because of the **softmax**. As soon as the size of the vocabulary increases, it becomes impossible to compute. In order to make the calculations faster, we change the objective and use the method of *negative sampling* (or, very close to it, the *noise contrastive estimation*).\n",
    "\n",
    "\n",
    "If we note $\\mathcal{D}$ the data set and we note $\\mathcal{D}'$ a set of pairs of words that are **not** in the data (and that in practice, we draw randomly), the objective is:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j \\in \\mathcal{D}}-\\log\\sigma(w_{i}c_{j}) + \\sum_{i, j \\in \\mathcal{D}'}\\log\\sigma(w_{i}c_{j})$$\n",
    "\n",
    "\n",
    "where $\\sigma$ is the sigmoid activation function $\\frac{1}{1 + \\exp(-x)}$.\n",
    "A common practice is to generate pairs from $\\mathcal{D}'$ in proportion to the frequencies of the words in the training data (the so-called unigram distribution):\n",
    "\n",
    "\n",
    "$$P(w) = \\frac{\\textbf{T}(w)^{0.75}}{\\sum_{w'\\in V} \\textbf{T}(w')}$$\n",
    "\n",
    "\n",
    "Although different, this new objective function is a sufficient approximation of the previous one, and is based on the same principle. Much research has been done on this objective: for example, [Levy and Golberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) shows that the objective calculates the PMI matrix shifted by a constant value. One can also see [Cotterell et al. 2017](https://aclanthology.coli.uni-saarland.de/papers/E17-2028/e17-2028) for an interpretation of the algorithm as a variant of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the ```gensim``` library for its implementation of word2vec in python. We'll have to make a specific use of it, since we want to keep the same vocabulary as before: we'll first create the class, then get the vocabulary we generated above. \n",
    "To avoid having to put all the data in memory all at once, we define a generator, which will take all the input data and pre-process it, and return to the ```Word2Vec``` class sentence by sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(vector_size=300,\n",
    "                 window=5,\n",
    "                 null_word=len(word_counts_5k))\n",
    "model.build_vocab_from_freq(word_counts_5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_corpus = [nltk.word_tokenize(texts[i]) for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Story',\n",
       " 'of',\n",
       " 'a',\n",
       " 'man',\n",
       " 'who',\n",
       " 'has',\n",
       " 'unnatural',\n",
       " 'feelings',\n",
       " 'for',\n",
       " 'a',\n",
       " 'pig',\n",
       " '.',\n",
       " 'Starts',\n",
       " 'out',\n",
       " 'with',\n",
       " 'a',\n",
       " 'opening',\n",
       " 'scene',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'terrific',\n",
       " 'example',\n",
       " 'of',\n",
       " 'absurd',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'A',\n",
       " 'formal',\n",
       " 'orchestra',\n",
       " 'audience',\n",
       " 'is',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'an',\n",
       " 'insane',\n",
       " ',',\n",
       " 'violent',\n",
       " 'mob',\n",
       " 'by',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'chantings',\n",
       " 'of',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'singers',\n",
       " '.',\n",
       " 'Unfortunately',\n",
       " 'it',\n",
       " 'stays',\n",
       " 'absurd',\n",
       " 'the',\n",
       " 'WHOLE',\n",
       " 'time',\n",
       " 'with',\n",
       " 'no',\n",
       " 'general',\n",
       " 'narrative',\n",
       " 'eventually',\n",
       " 'making',\n",
       " 'it',\n",
       " 'just',\n",
       " 'too',\n",
       " 'off',\n",
       " 'putting',\n",
       " '.',\n",
       " 'Even',\n",
       " 'those',\n",
       " 'from',\n",
       " 'the',\n",
       " 'era',\n",
       " 'should',\n",
       " 'be',\n",
       " 'turned',\n",
       " 'off',\n",
       " '.',\n",
       " 'The',\n",
       " 'cryptic',\n",
       " 'dialogue',\n",
       " 'would',\n",
       " 'make',\n",
       " 'Shakespeare',\n",
       " 'seem',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'a',\n",
       " 'third',\n",
       " 'grader',\n",
       " '.',\n",
       " 'On',\n",
       " 'a',\n",
       " 'technical',\n",
       " 'level',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'better',\n",
       " 'than',\n",
       " 'you',\n",
       " 'might',\n",
       " 'think',\n",
       " 'with',\n",
       " 'some',\n",
       " 'good',\n",
       " 'cinematography',\n",
       " 'by',\n",
       " 'future',\n",
       " 'great',\n",
       " 'Vilmos',\n",
       " 'Zsigmond',\n",
       " '.',\n",
       " 'Future',\n",
       " 'stars',\n",
       " 'Sally',\n",
       " 'Kirkland',\n",
       " 'and',\n",
       " 'Frederic',\n",
       " 'Forrest',\n",
       " 'can',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'briefly',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95923759, 208640790)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(preprocessed_corpus, total_examples=25000, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.18454788e-03, -1.37849525e-03, -1.01620320e-03, -9.02454834e-04,\n",
       "        1.05600001e-03, -2.16791872e-03, -2.05645571e-03, -3.55383963e-04,\n",
       "        3.16166878e-03, -2.47142161e-03,  3.17959953e-03, -2.36463593e-03,\n",
       "       -1.14690547e-03,  5.17934561e-04,  2.09178240e-03,  1.65859575e-03,\n",
       "       -3.32989660e-03,  2.66940426e-03,  9.63700586e-04, -2.66272947e-03,\n",
       "       -2.65578856e-03,  2.88256328e-03,  1.23786961e-03,  2.10215058e-03,\n",
       "        5.99134364e-04,  3.17870383e-03, -5.04287076e-04,  7.69764185e-04,\n",
       "       -1.34191592e-03, -2.21518194e-03,  1.89394632e-03, -1.39200606e-03,\n",
       "        5.47342293e-04, -3.07131617e-04,  5.83800487e-04,  2.62877933e-04,\n",
       "       -8.37150787e-04, -3.23931291e-03,  2.52716942e-03,  9.39277001e-04,\n",
       "       -3.09186376e-04,  1.09345512e-03, -2.75015505e-03,  1.68446463e-03,\n",
       "       -2.68779392e-03,  1.99498376e-03, -2.20374344e-03, -2.69673951e-03,\n",
       "       -3.11120902e-03,  2.04317202e-03,  1.30404590e-03, -1.53276604e-03,\n",
       "        2.60033761e-03, -2.13976414e-03,  1.80641294e-03, -1.23336597e-03,\n",
       "        9.24282067e-04, -1.58293243e-03, -2.20514485e-03,  3.29388393e-04,\n",
       "        1.99935865e-03,  2.50817765e-03,  2.04183301e-03,  2.86521832e-03,\n",
       "        3.05016432e-03, -3.16946628e-03,  1.04776106e-03, -9.90206376e-04,\n",
       "       -3.52838833e-04, -1.25350629e-03, -5.38870867e-04, -4.89214668e-04,\n",
       "        3.14258691e-03, -1.10209349e-03, -3.29496828e-03, -2.11278512e-03,\n",
       "       -3.12270643e-03,  2.96834181e-03, -1.10286870e-04, -1.43204525e-03,\n",
       "       -6.18237653e-04,  2.24437006e-03, -1.99731346e-03,  2.03263131e-03,\n",
       "        2.50913855e-03, -1.36014505e-03, -1.06736505e-03, -2.49630259e-03,\n",
       "        3.27880098e-03, -2.66956282e-03,  2.34382390e-03,  1.15865073e-03,\n",
       "       -5.06547280e-04, -4.02629375e-04,  2.95770285e-03, -3.10900807e-03,\n",
       "        1.88927329e-03,  1.26106699e-03, -2.94645899e-03, -1.13745926e-04,\n",
       "       -8.26453790e-04, -1.92613562e-03,  2.55894335e-03, -9.83913706e-05,\n",
       "        6.49793947e-04,  7.51321728e-04,  2.02918123e-03,  8.85077345e-04,\n",
       "        2.07157014e-03,  1.68447767e-03,  3.71501839e-04,  1.05090777e-03,\n",
       "       -2.16250331e-03, -2.28009420e-03,  1.05524738e-03, -6.92490721e-04,\n",
       "       -8.88528826e-04,  1.46258832e-03,  7.59637333e-05, -6.35716133e-04,\n",
       "       -5.61094261e-04, -2.71257129e-03,  1.26392092e-03, -1.93774704e-05,\n",
       "        6.68437860e-04, -2.63511925e-03, -1.46561861e-03, -2.37884163e-03,\n",
       "        5.09876816e-04,  1.32947322e-03, -2.77637923e-03, -1.83009938e-03,\n",
       "       -1.98932854e-03, -2.41625030e-03, -2.97746295e-03,  2.23595253e-03,\n",
       "       -7.24860001e-05, -1.76410517e-03,  9.76298645e-04, -1.68839016e-03,\n",
       "       -1.79291051e-03, -8.52917845e-04,  3.07184295e-03, -2.83008208e-03,\n",
       "       -1.57873635e-03, -7.30447355e-04,  5.59929991e-04, -3.13030882e-03,\n",
       "       -2.20102869e-04, -3.16794286e-03, -7.87115889e-04, -1.71384297e-03,\n",
       "       -2.47225212e-03, -1.85681658e-03,  1.56457978e-03, -1.12373789e-03,\n",
       "        1.29709567e-03,  3.29125370e-03, -1.34927745e-03, -1.94640554e-04,\n",
       "       -9.81300254e-04,  3.71123169e-04, -2.90433853e-03,  5.55278442e-04,\n",
       "        7.34768691e-04, -2.57771858e-03,  1.65626686e-03, -1.55896309e-03,\n",
       "       -2.54904036e-03, -3.31296399e-03, -1.65978784e-03,  2.73389067e-03,\n",
       "       -3.27734277e-03,  1.37592037e-03,  1.94093620e-03,  1.07766909e-03,\n",
       "        3.13972961e-03,  9.20784485e-04, -6.82690123e-04, -2.89596361e-03,\n",
       "        3.03719565e-03, -1.35385117e-03, -5.10893646e-04, -2.89354357e-03,\n",
       "       -5.19412744e-04, -8.68479430e-04, -4.03800019e-04,  2.82948208e-03,\n",
       "        8.39236949e-04,  1.23428344e-03, -1.88995281e-03, -5.77085419e-04,\n",
       "       -2.71963608e-03, -9.52316099e-04, -1.11337309e-03, -7.61805801e-04,\n",
       "        2.34625346e-04, -2.78619817e-03, -5.45482209e-04,  1.23363500e-03,\n",
       "        2.22393940e-03,  1.23793003e-03,  1.43648780e-04,  1.61667785e-03,\n",
       "        1.31158950e-03, -6.81846170e-04, -1.57068964e-04, -2.98680476e-04,\n",
       "       -1.32501160e-03,  1.83015864e-03, -2.25554942e-03,  4.09761677e-04,\n",
       "        2.72771483e-03,  1.15335267e-03, -1.46502489e-03,  3.67895758e-04,\n",
       "        3.11885402e-03, -2.18273795e-04, -2.73565529e-03,  2.73809978e-03,\n",
       "        1.72258890e-03,  2.39257817e-03,  1.66236714e-03,  3.11727682e-03,\n",
       "       -2.05333671e-03, -2.18496914e-03,  4.03341459e-04, -2.32127542e-03,\n",
       "        3.26104672e-03, -2.17946377e-04,  2.43047439e-03, -2.81869620e-03,\n",
       "       -3.29811563e-04,  6.20755774e-04, -2.96018086e-03,  1.79733557e-03,\n",
       "        4.71933279e-04,  2.57512322e-03,  8.34571896e-04,  5.15362830e-04,\n",
       "       -1.47665467e-03,  1.80704199e-04, -2.51675164e-03,  8.15252832e-04,\n",
       "       -3.21166427e-03, -3.29813641e-03,  6.96789415e-04, -3.08128633e-03,\n",
       "       -2.13335198e-03, -2.16325920e-04,  3.15648271e-03, -7.24142010e-04,\n",
       "        1.39447337e-03, -6.98418589e-04, -2.98598083e-03, -1.15998031e-03,\n",
       "        2.21506553e-03, -1.53859332e-03, -2.77765351e-03, -4.92597814e-04,\n",
       "        2.94104544e-03,  2.50175392e-04, -1.67117198e-03, -1.70730788e-03,\n",
       "        5.32630656e-04,  7.93858344e-05,  2.79081496e-03,  2.32272223e-03,\n",
       "        1.93249306e-03,  2.17930996e-03,  2.16500761e-04, -1.77747686e-03,\n",
       "        2.16406188e-03, -3.28014023e-03, -7.27707520e-04, -2.84543401e-03,\n",
       "       -4.26516519e-04,  3.21308384e-04,  2.99272942e-04,  4.38543968e-04,\n",
       "       -2.45679100e-03,  1.34129962e-03,  2.26265914e-03,  4.56976879e-04,\n",
       "       -3.27237532e-03, -9.35077260e-04, -2.57437746e-03,  5.25148294e-04,\n",
       "        2.33633164e-03,  2.66749342e-03,  2.93761725e-03,  1.21242402e-03,\n",
       "       -2.86873709e-03,  2.60185800e-04, -1.15224044e-03,  2.22661183e-03,\n",
       "       -3.30202573e-04, -5.15216962e-04, -1.77455344e-03, -3.18592554e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2VEmbeddings = model.wv.vectors\n",
    "print(W2VEmbeddings.shape)\n",
    "W2VEmbeddings[vocab_5k['UNK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['great', 'bad', 'decent', 'opens', 'wong', 'cynical', 'terrific', 'hack', 'ok']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['great', 'bad', 'decent', 'basic', 'opens', 'nice', 'fine', 'terrific', 'reality']]\n"
     ]
    }
   ],
   "source": [
    "print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n",
    "print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to sentiment analysis\n",
    "\n",
    "We will now use these representations for sentiment analysis. \n",
    "The basic model, as before, will be constructed in two steps:\n",
    "- A function to obtain vector representations of criticism, from text, vocabulary, and vector representations of words. Such a function (to be completed below) will associate to each word of a review its embeddings, and create the representation for the whole sentence by summing these embeddings.\n",
    "- A classifier will take these representations as input and make a prediction. To achieve this, we can first use logistic regression ```LogisticRegression``` from ```scikit-learn```  \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
    "    \"\"\"\n",
    "    Represent the sentences as a combination of the vector of its words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : a list of sentences   \n",
    "    vocabulary : dict\n",
    "        From words to indexes of vector.\n",
    "    embeddings : Matrix containing word representations\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
    "    \"\"\"\n",
    "    # Initialize an empty array to store the sentence representations\n",
    "    representations = np.zeros((len(texts), embeddings.shape[1]))\n",
    "\n",
    "    # Loop through each sentence\n",
    "    for i, sentence in enumerate(texts):\n",
    "        # Split the sentence into words\n",
    "        sent = clean_and_tokenize(sentence)\n",
    "        # Obtenir les indexs de la phrase grace au vocabulaire: \n",
    "        sent_idx = []\n",
    "        for word in sent:\n",
    "            if vocabulary.get(word) != None : \n",
    "                sent_idx.append(vocabulary.get(word))\n",
    "            else:\n",
    "                sent_idx.append(vocabulary.get('UNK'))\n",
    "\n",
    "        sent_repr = np.zeros((len(sent), embeddings.shape[1]))\n",
    "        for j,idx in enumerate(sent_idx):\n",
    "            sent_repr[j,:] = embeddings[idx,:]\n",
    "\n",
    "        # Apply the numpy operation to the sentence representation\n",
    "        representations[i] = np_func(sent_repr, axis=0)\n",
    "        \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8309999999999998 (std 0.004766550115125172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Exemple avec les embeddings obtenus via Glove\n",
    "rep = sentence_representations(texts, vocab_5k, GloveEmbeddings)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.62212 (std 0.009050613238891603)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Exemple avec les embeddings obtenus via SVD\n",
    "rep = sentence_representations(texts, vocab_5k, SVDEmbeddings)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.83008 (std 0.002643028565869106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Exemple avec les embeddings obtenus via Word2vec\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "\n",
    "You can know compare the various sets of embeddings, and answer the following questions:\n",
    "- Why could we expect that the results obtained with embeddings pre-trained with Glove are better than others ? Is that verified ? And what changes if we remove the constraint of having the same 5000 words vocabulary as the other methods ? \n",
    "- Which matrix that we can reduce the dimension of gives the best results ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we could expect better result with Glove is that it is pre-trained on a huge amount of data, so it could take into account semantic and syntactic words' relationships in order to get good representations. It is often used in NLP tasks because it doesn't take into account only the co-occurence of words but it also considers the importance of the pointwise mutual information. \\\n",
    "Yes it verified as it has the highest score (0.83864) of the sentence representation compared to the other embeddings methods (SVD : 0.62, Word2Vec : 0.83312).\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93231, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_all, word_counts_all = vocabulary(texts, 0, 0)\n",
    "GloveEmbeddings_all = get_glove_adapted_embeddings(loaded_glove_model, vocab_all)\n",
    "print(GloveEmbeddings_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.83088 (std 0.008500917597530295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Exemple avec les embeddings obtenus via Glove\n",
    "rep = sentence_representations(texts, vocab_all, GloveEmbeddings_all)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing really changes, we still have the same score of 0.83 when we change the 5000 words vocabulary constraint to 3000 or 10000. Even when we remove the constraint of words limit, there is no major change in the quality of the sentence representation. Glove seems to not care about words with lower frequencies as adding them does not change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8286399999999998 (std 0.005471964912168178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=200)\n",
    "SVDEmbeddings_glove = svd.fit_transform(GloveEmbeddings)\n",
    "rep = sentence_representations(texts, vocab_5k, SVDEmbeddings_glove)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8193999999999999 (std 0.002920273959751053)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouedr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=200)\n",
    "SVDEmbeddings_w2v = svd.fit_transform(W2VEmbeddings)\n",
    "rep = sentence_representations(texts, vocab_5k, SVDEmbeddings_w2v)\n",
    "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
    "print(clf.score(rep[1::2], y[1::2]))\n",
    "\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS we tried to run glove and word2vec embeddings while reducing the dimension to 200 features, we can notice that he Glove matrix gives the best results with and without this reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity\n",
    "\n",
    "We will now use these representations for Semantic Textual Similarity, following the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = {'train':dict(), 'test':dict()}\n",
    "    for fn in os.listdir(path):\n",
    "        if fn.endswith(\".csv\"):\n",
    "            with open(os.path.join(path, fn)) as f:\n",
    "                subset = fn[:-4].split(\"-\")[1]        \n",
    "                if subset == \"dev\":\n",
    "                    subset = \"train\"        \n",
    "                data[subset]['data'] = []\n",
    "                data[subset]['scores'] = []\n",
    "                for l in f:          \n",
    "                    l = l.strip().split(\"\\t\")          \n",
    "                    data[subset]['data'].append((l[5],l[6]))\n",
    "                    data[subset]['scores'].append(float(l[4]) / 5) # mapping the score to the 0-1 range \n",
    "    return data\n",
    "\n",
    "sts_dataset = load_data(path=\"stsbenchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is now, again, to:\n",
    "- Obtain vector representations of the sentences\n",
    "- Compute their cosine distance\n",
    "- Fit a regressor to predict the similarity score\n",
    "- Predict the scores for the test data and compare it to the gold scores using Spearman correlation (```spearmanr```)\n",
    "\n",
    "To obtain vector representations, use the functions we previously created used (notably ```sentence_representations```), and experiment with:\n",
    "- the co-occurence matrix\n",
    "- the PPMI matrix\n",
    "- the PPMI matrix SVD-reduced\n",
    "- the glove vectors\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11319\n"
     ]
    }
   ],
   "source": [
    "# build a vocabulary from the STS training set\n",
    "corps = []\n",
    "for tupl in sts_dataset['train']['data']:\n",
    "    corps.append(tupl[0])\n",
    "    corps.append(tupl[1])\n",
    "\n",
    "sts_voc, sts_counts = vocabulary(corps,0,20000)\n",
    "# Create a co-occurrence matrix\n",
    "M_co_oc_sts = co_occurence_matrix(corps, sts_voc, 5, True)\n",
    "print(len(sts_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sts_dataset['train']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['memorials', 'healthpark', 'skijoring', 'maintains', 'meditating', 'praying', 'hiking', 'mooing', 'levitating']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['memorials', 'healthpark', 'suspicions', 'discussions', 'engage', 'maintains', 'campus', 'confusing', 'listed']]\n",
      "(10001, 300)\n",
      "Plus proches voisins de good selon la distance 'euclidean': \n",
      "[['nugent', 'mental', 'graham', 'brazen', 'listed', 'discovery', 'firefighter', 'foxe', 'richard']]\n",
      "Plus proches voisins de good selon la distance 'cosine': \n",
      "[['nugent', 'neck', 'established', 'listed', 'firefighter', 'mental', 'discovery', 'registrars', 'graham']]\n"
     ]
    }
   ],
   "source": [
    "# Transform the matrix in different ways (for example, here: PMI weighting and dim reduction with SVD)\n",
    "pmi_sts = pmi(M_co_oc_sts)\n",
    "print_neighbors(euclidean, sts_voc, pmi_sts, 'good')\n",
    "print_neighbors(cosine, sts_voc, pmi_sts, 'good')\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "SVDEmbeddings = svd.fit_transform(M_co_oc_sts)\n",
    "print(SVDEmbeddings.shape)\n",
    "SVDEmbeddings[sts_voc['UNK']]\n",
    "\n",
    "print_neighbors(euclidean, sts_voc, SVDEmbeddings, 'good')\n",
    "print_neighbors(cosine, sts_voc, SVDEmbeddings, 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the sentence pair representations and calculate their cosine distances to be used as features\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['train']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, SVDEmbeddings)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, SVDEmbeddings)\n",
    "\n",
    "X_train = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_train.append(cos_dist)\n",
    "y_train = []\n",
    "for label in sts_dataset['train']['scores']:\n",
    "    y_train.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 1)\n",
      "(5749, 1)\n",
      "(1379, 1)\n",
      "(1379, 1)\n",
      "SignificanceResult(statistic=0.02726113028096696, pvalue=0.31172502489361603)\n"
     ]
    }
   ],
   "source": [
    "# Train a linear regression model, make predictions on test set, evaluate it using spearman's r\n",
    "lr = LinearRegression()\n",
    "X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "### TEST\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['test']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, SVDEmbeddings)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, SVDEmbeddings)\n",
    "\n",
    "X_test = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_test.append(cos_dist)\n",
    "y_test = []\n",
    "for label in sts_dataset['test']['scores']:\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.asarray(X_test).reshape(-1, 1)\n",
    "y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(spearmanr(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 1)\n",
      "(5749, 1)\n",
      "(1379, 1)\n",
      "(1379, 1)\n",
      "SignificanceResult(statistic=0.245264291314739, pvalue=2.4396378856958903e-20)\n"
     ]
    }
   ],
   "source": [
    "# Same with PPMI matrix\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['train']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, pmi_sts)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, pmi_sts)\n",
    "\n",
    "X_train = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_train.append(cos_dist)\n",
    "y_train = []\n",
    "for label in sts_dataset['train']['scores']:\n",
    "    y_train.append(label)\n",
    "\n",
    "### TEST\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['test']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, pmi_sts)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, pmi_sts)\n",
    "\n",
    "X_test = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_test.append(cos_dist)\n",
    "y_test = []\n",
    "for label in sts_dataset['test']['scores']:\n",
    "    y_test.append(label)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_test = np.asarray(X_test).reshape(-1, 1)\n",
    "y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(spearmanr(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 1)\n",
      "(5749, 1)\n",
      "(1379, 1)\n",
      "(1379, 1)\n",
      "SignificanceResult(statistic=0.1470994408850802, pvalue=4.0779152014904285e-08)\n"
     ]
    }
   ],
   "source": [
    "# Same with PPMI SVD-reduced matrix\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "SVDEmbeddings_pmi = svd.fit_transform(pmi_sts)\n",
    "\n",
    "\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['train']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, SVDEmbeddings_pmi)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, SVDEmbeddings_pmi)\n",
    "\n",
    "X_train = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_train.append(cos_dist)\n",
    "y_train = []\n",
    "for label in sts_dataset['train']['scores']:\n",
    "    y_train.append(label)\n",
    "\n",
    "### TEST\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['test']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, SVDEmbeddings_pmi)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, SVDEmbeddings_pmi)\n",
    "\n",
    "X_test = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_test.append(cos_dist)\n",
    "y_test = []\n",
    "for label in sts_dataset['test']['scores']:\n",
    "    y_test.append(label)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_test = np.asarray(X_test).reshape(-1, 1)\n",
    "y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(spearmanr(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 1)\n",
      "(5749, 1)\n",
      "(1379, 1)\n",
      "(1379, 1)\n",
      "SignificanceResult(statistic=0.44920794001518977, pvalue=1.9373655546337143e-69)\n"
     ]
    }
   ],
   "source": [
    "# And lastly, with Glove vectors\n",
    "GloveEmbeddings_sts = get_glove_adapted_embeddings(loaded_glove_model, sts_voc)\n",
    "\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['train']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, GloveEmbeddings_sts)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, GloveEmbeddings_sts)\n",
    "\n",
    "X_train = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_train.append(cos_dist)\n",
    "y_train = []\n",
    "for label in sts_dataset['train']['scores']:\n",
    "    y_train.append(label)\n",
    "\n",
    "### TEST\n",
    "corps_a = []\n",
    "corps_b = []\n",
    "for tupl in sts_dataset['test']['data']:\n",
    "    corps_a.append(tupl[0])\n",
    "    corps_b.append(tupl[1])\n",
    "rep_a = sentence_representations(corps_a, sts_voc, GloveEmbeddings_sts)\n",
    "rep_b = sentence_representations(corps_b, sts_voc, GloveEmbeddings_sts)\n",
    "\n",
    "X_test = []\n",
    "for i in range(len(corps_a)):\n",
    "    cos_dist = cosine(rep_a[i], rep_b[i])\n",
    "    X_test.append(cos_dist)\n",
    "y_test = []\n",
    "for label in sts_dataset['test']['scores']:\n",
    "    y_test.append(label)\n",
    "\n",
    "\n",
    "X_train = np.asarray(X_train).reshape(-1, 1)\n",
    "y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_test = np.asarray(X_test).reshape(-1, 1)\n",
    "y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(spearmanr(preds, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to spearman correlation results, we can verify that Glove embeddings performs well and better than the other methods.\n",
    "Its statistic value 0.44 is the closest to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
